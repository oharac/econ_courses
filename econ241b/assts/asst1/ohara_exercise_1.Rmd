---
title: | 
  | \hfill \Large{Econ241b: PS 1}
author: |
  | \hfill Casey O'Hara
date: |
  | \hfill `r Sys.Date()`
output:
  pdf_document:
    toc: no
  html_document:
    toc: no
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---

\begin{enumerate}
\item % question 1
  \begin{enumerate}
  \item %a
    If $\mathbb{E}[y|x] = \beta_1 x + \beta_0$, find $\mathbb{E}[yx]$ as a function of the moments of $x$.
    \begin{align*}
      y &= \mathbb{E}[y|x] + u = \beta_1 x + \beta_0 + u        
          &\text{(standard CEF)}\\
      \mathbb{E}[yx] &= \mathbb{E} [(\beta_1 x + \beta_0 + u)x] 
          &\text{(substitute)}\\
        &= \mathbb{E} [\beta_1 x^2 + \beta_0 x + ux] 
          &\text{(distribute)}\\
        &= \mathbb{E} [\beta_1 x^2] + \mathbb{E} [\beta_0 x] + \mathbb{E} [ux] 
          &\text{(linearity of $\mathbb{E}[\cdot]$)}\\
        &= \beta_1 \mathbb{E}[x^2] + \beta_0 \mathbb{E} [x] + \mathbb{E} [ux] 
          &\text{(pull out constants)}\\
      \mathbb{E} [ux] &= \mathbb{E}[\mathbb{E}[ux|x]] = 
          \mathbb{E}[x \mathbb{E}[u|x]] = 0 = \mathbb{E}[x \cdot 0] = 0 
          &\text{($u, x$ uncorr.; $\mathbb{E}[u|x] = 0$ by constr.)}\\
      \Rightarrow \mathbb{E}[yx] &= \beta_1 \mathbb{E}[x^2] + \beta_0 \mathbb{E} [x]
          &\blacksquare
    \end{align*}
    First two moments of $x$ are $\mathbb{E} [x]$ and $\mathbb{E}[x^2]$.
    
  \item %b
    Suppose the random variables $y$ and $x$ take only the values 0 and 1 and
    have the following joint probability distribution:
    \begin{table}[h!]
      \centering
      \label{tab:table1}
      \begin{tabular}{l|c|c|c}
                &  $x=0$ &  $x=1$\\
        \hline
        $y=0$   &    a   &    c \\
        $y=1$   &    b   &    d \\
      \end{tabular}
    \end{table}
    \begin{itemize}
      \item 
        To satisfy the properties of a joint distribution, what must be true of (a; b; c; d)?
        \begin{itemize}
          \item Total probability must add to 1: $a + b + c + d = 1$.
          \item Each probability must be positive: $a, b, c, d \geq 0$.
        \end{itemize}
      \item
        Find $\mathbb{E}[y|x]$, $\mathbb{E}[y^2|x]$, and $Var(y|x)$ for $x = 0$ and $x = 1$.
        
        Note that $a = P(y = 0, x = 0)$, $b = P(y = 1, x = 0)$,  
          $c = P(y = 0, x = 1)$,  $d = P(y = 1, x = 1)$.
        From this, we know $P(x = 0) = a + b, P(x = 1) = c + d)$.  
        
        Conditional probability: 
        $$P(y = j | x = k) = \frac{P(y = j, x = k)}{P(x = k)}$$

        Conditional expected value is sum of conditional probabilities for each value of y at the given value of x:
        $$\mathbb{E}[y|x = k] = \sum_{j = 0}^1\frac{P(y = j, x = k)}{P(x = k)}$$
        
        From this, calculate $\mathbb{E}[y|x]$ for $x = 0$ and $x = 1$:

        $$\mathbb{E}[y|x=0] = \frac{0 \cdot a}{a + b} + \frac{1 \cdot b}{a + b} = \frac{b}{a + b}; \hspace{20pt}
          \mathbb{E}[y|x=1] = \frac{d}{c + d}$$
        
        Since $y$ only takes values of 0 and 1, and $y = y^2$ for each of 
        these, then $\mathbb{E}[y^2|x]$ = $\mathbb{E}[y|x]$ in each case.
        
        $$Var(y|x = 0) = \mathbb{E}[y^2|x = 0] - \mathbb{E}[y|x = 0]^2 = \frac{b^2}{(a + b)^2}; \hspace{12pt}
          Var(y|x = 1) = \frac{d^2}{(c + d)^2}$$
        
    \end{itemize}
  \end{enumerate}

\item %question 2
  Assume $\mathbb{E} |g(x) y| < \infty$:
  
  Prove
  
  $$\mathbb{E} [g(x) y|x] = g(x) \mathbb{E} [y|x]$$

  \begin{align*}
    \mathbb{E} [g(x) y|x] &=  \int_{-\infty}^{\infty} g(x) y f_{y|x}(y|x) dy
        &\text{(def of conditional expectation)}\\
      &= g(x) \int_{-\infty}^{\infty} y f_{y|x}(y|x) dy
        &\text{($g(x)$ not function of $y$)}\\
      &= g(x) \mathbb{E} [y|x]
        &\text{(def of conditional expectation) }\blacksquare
  \end{align*}
  
  
\item %question 3
  If $y = x\beta + u$, $x^2 \in \mathbb{R}$, then for each of the following statements either
  establish that they are true or provide a counterexample:
  \begin{enumerate}
    \item $\mathbb{E} [u|x] = 0$ implies $\mathbb{E} [x^2u] = 0$\\
      \begin{align*}
        \mathbb{E} [x^2u] &= \mathbb{E} [\mathbb{E} [x^2u | x] ]
            &\text{(iterated expectations)}\\
          &= \mathbb{E} [x^2 \mathbb{E} [u | x] ]
            &\text{(conditionining thm)}\\
          &= \mathbb{E} [x^2 \cdot 0]
            &\text{(given $\mathbb{E} [u|x] = 0$)}\\
        \mathbb{E} [x^2u] &= 0 &\blacksquare
      \end{align*}
          
    \item $\mathbb{E} [xu]  = 0$ implies $\mathbb{E} [x^2u] = 0$\\
    
      For CEF, distribution of $u$ constructed such that 
      $\mathbb{E}[u] = 0$, but that is not given to us here.  Suppose, toward 
      contradiction, that $u ~ n(1, 1)$ (and thus $\mathbb{E}[u] = 1$, and
      $u$ is independent of $x$). Further,
      suppose $x ~ uniform(-1, 1)$, so $\mathbb{E}[x] = 0$ and 
      $\mathbb{E}[x^2] = 1/3$.
      
      \begin{align*}
        \mathbb{E} [xu]  &= \mathbb{E} [\mathbb{E} [xu | x]]
            &\text{(law of iterated expectations)}\\
          &= \mathbb{E} [x \mathbb{E} [u | x]]
            &\text{(conditioning thm)}\\
        \mathbb{E} [u | x] &= \mathbb{E} [u] = 1
            &\text{(from supposition)}\\
        \mathbb{E} [xu]  &= \mathbb{E} [x \cdot 1]
            &\text{(substitute)}\\
          &= \mathbb{E} [x] = 0
            &\text{(from supposition) } \square \\[12pt]
        \mathbb{E} [x^2 u]  &= \mathbb{E} [\mathbb{E} [x^2 u | x]]
            &\text{(l.i.e.)}\\
          &= \mathbb{E} [x^2 \mathbb{E} [u | x]]
            &\text{(conditioning thm)}\\
        \mathbb{E} [x^2 u]  &= \mathbb{E} [x \cdot 1]
            &\text{(substitute)}\\
          &= \mathbb{E} [x^2] = 1/3 \neq 0
            &\text{(from supposition) } \blacksquare
      \end{align*}
      
      Having identified a counterexample, we can see that 
      $\mathbb{E} [xu]  = 0$ does not imply $\mathbb{E} [x^2u] = 0$.
      
    \item $\mathbb{E} [u|x] = 0$ implies $\mathbb{E} [y|x] = x\beta$\\
      \begin{align*}
        y &= x\beta + u 
            &\text{(given)}\\
        \mathbb{E}[y | x] &= \mathbb{E}[x\beta + u | x] 
            &\text{(substitute)}\\
          &= \mathbb{E}[x\beta | x]  + \mathbb{E} [u | x] 
            &\text{(linearity of $\mathbb{E}[\cdot]$)}\\
          &= \beta \mathbb{E}[x | x]  + 0 
            &\text{(lin. of $\mathbb{E}[\cdot]$ and subst.)}\\
          &= \beta x
            &\text{(conditioning thm) } \blacksquare
      \end{align*}
    
    \item $\mathbb{E} [xu]  = 0$ implies $\mathbb{E} [y|x] = x\beta$\\
      Suppose toward contradiction that $u ~ n(1, 1)$ and $x ~ uniform(-1, 1)$.
      As above, we can confirm that $\mathbb{E} [xu]  = 0$ and 
      $\mathbb{E} [u | x] = \mathbb{E} [u] = 1$ under these conditions.
      
      \begin{align*}
        y &= x\beta + u 
            &\text{(given)}\\
        \mathbb{E}[y | x] &= \mathbb{E}[x\beta + u | x] 
            &\text{(substitute)}\\
          &= \mathbb{E}[x\beta | x]  + \mathbb{E} [u | x] 
            &\text{(linearity of $\mathbb{E}[\cdot]$)}\\
          &= \beta \mathbb{E}[x | x]  + 1 
            &\text{(lin. of $\mathbb{E}[\cdot]$ and subst.)}\\
          &= \beta x + 1 \neq x \beta
            &\text{(conditioning thm) } \blacksquare
      \end{align*}
      Having identified a counterexample, we see that $\mathbb{E} [xu]  = 0$ 
      does not necessarily imply $\mathbb{E} [y|x] = x\beta$. 
      
  \end{enumerate}

\item %question 4
  Recall that the conditional variance is 
  
  $$\sigma^2(x) = Var (y|x) = \mathbb{E} [(y -\mathbb{E} [y|x])^2|x]$$
  Show that the conditional variance can be written as
  $$\sigma^2(x) = \mathbb{E} [y^2 | x] - \mathbb{E} [y | x]^2$$

  \begin{align*}
    \sigma^2(x) &= Var (y|x) = \mathbb{E} [(y - \mathbb{E} [y|x])^2|x]
        &\text{(given)}\\
      &= \mathbb{E} [(y^2 - 2y\mathbb{E} [y|x] + \mathbb{E} [y|x]^2)|x]
        &\text{(expand)}\\
      &= \mathbb{E} [y^2|x] - \mathbb{E} [2y\mathbb{E} [y|x] | x] + \mathbb{E} [\mathbb{E} [y|x]^2 | x]
        &\text{(lin. of $\mathbb{E}[\cdot]$)}\\[12pt]
    y &= \mathbb{E}[y | x] + e
        &\text{(def of CEF)}\\
    \Rightarrow \mathbb{E} [2y\mathbb{E} [y|x] | x] &= 2 \mathbb{E} [ (\mathbb{E}[y | x] + e)\mathbb{E}[y | x] | x]
        &\text{(substitution)}\\
      &= 2 \mathbb{E} [\mathbb{E}[y | x]^2 + e\mathbb{E}[y | x] | x]
        &\text{(substitution)}\\
      &= 2 \mathbb{E} [\mathbb{E}[y | x]^2 | x] + 2 \mathbb{E}[e \mathbb{E}[y | x] | x]
        &\text{(lin. of $\mathbb{E}[\cdot]$)}\\
    \text{But } \mathbb{E}[e \mathbb{E}[y | x] | x] &= \mathbb{E}[e | x] \mathbb{E}[\mathbb{E}[y | x] | x] = 0
        &\text{($\mathbb{E}[e | x] = 0$ by constr.)}\\
    \Rightarrow \mathbb{E} [2y\mathbb{E} [y|x] | x] &= 2 \mathbb{E} [\mathbb{E}[y | x]^2 | x] + 0
        &\text{(substitution)}\\
      &= 2 \mathbb{E} [y | x]^2
        &\text{(l.i.e.)}\\[12pt]
    \Rightarrow \sigma^2(x) &= \mathbb{E} [y^2|x] -  2 \mathbb{E} [y | x]^2 + \mathbb{E} [y | x]^2
        &\text{(subst., l.i.e.)}\\
    \Rightarrow \sigma^2(x) &= \mathbb{E} [y^2|x] - \mathbb{E} [y | x]^2
        &\blacksquare\\
  \end{align*}
  

\end{enumerate}